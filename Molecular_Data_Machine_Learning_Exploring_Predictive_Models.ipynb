{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Molecular Data Machine Learning: Exploring Predictive Models\n",
        "\n",
        "This notebook explores the \"Molecular Data Machine Learning\" Kaggle competition,\n",
        "focusing on predicting molecular properties (coupling constants). We'll load\n",
        "and examine the data, perform feature engineering, visualize relationships,\n",
        "and train several regression models, including linear models, tree-based\n",
        "methods, and ensemble techniques. The goal is to build a robust and accurate\n",
        "predictive model, keeping in mind the dataset's characteristics of having\n",
        "relatively few rows but many features.\n",
        "\n",
        "Author: AI Assistant (based on user's request)\n",
        "\"\"\"\n",
        "\n",
        "# ## 1. Importing Necessary Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import lightgbm as lgb  # You might need to install this: pip install lightgbm\n",
        "import xgboost as xgb  # You might need to install this: pip install xgboost\n",
        "from rdkit import Chem #You might need to install this: conda install -c conda-forge rdkit\n",
        "\n",
        "# ## 2. Loading the Data\n",
        "\n",
        "# Load the training data. For demonstration purposes, we'll assume the data\n",
        "# is in 'train.csv'. You would replace this with the actual path to the training data.\n",
        "try:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    print(\"Training data loaded successfully.\")\n",
        "    print(f\"Shape of training data: {train_df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: train.csv not found. Please ensure the file is in the correct directory.\")\n",
        "    # For demonstration, let's create a dummy DataFrame with the discussed structure\n",
        "    n_rows = 100  # Simulate 'few rows'\n",
        "    n_cols = 50  # Simulate 'many columns'\n",
        "    dummy_data = np.random.rand(n_rows, n_cols)\n",
        "    dummy_cols = [f'feature_{i}' for i in range(n_cols)] + ['target', 'molecule_structure'] # Added dummy column\n",
        "    train_df = pd.DataFrame(np.hstack((dummy_data, np.random.rand(n_rows, 2))), columns=dummy_cols)\n",
        "    train_df['molecule_structure'] = ['C1CCCCC1', 'C1=CC=CC=C1', 'CC(=O)C', 'C1CN2C(=O)C(=O)CC2', 'C1OC2OC(OC(C2C1O)n3c4ccccc4c5nc[nH]c3=O)C(O)(C(C(C6=O)OCC7OC(F)(F)C(F)(F)C(F)(F)C7F)(F)F)C(F)(F)F',\n",
        "                            'C1=CC2C(=CC(=O)C(=C2C(=O)O1)C3=CC=CC=C3)C4=CC=CC=C4',\n",
        "                            'C1CCCCCC1', 'C1=CC=CC=C1', 'CC(=O)C', 'C1CN2C(=O)C(=O)CC2'] * 10\n",
        "    print(\"Using a dummy training dataset for demonstration.\")\n",
        "\n",
        "# Assuming 'target' is the column we want to predict (replace with the actual target column name)\n",
        "TARGET_COL = 'target'\n",
        "FEATURES = [col for col in train_df.columns if col != TARGET_COL and col != 'molecule_structure'] # Exclude the molecule structure\n",
        "MOLECULE_COL = 'molecule_structure' # Define new variable\n",
        "\n",
        "# ## 3. Exploratory Data Analysis (EDA) - A Glimpse\n",
        "\n",
        "# Let's get a quick overview of the data.\n",
        "print(\"\\nFirst few rows of the training data:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nSummary statistics of the training data:\")\n",
        "print(train_df.describe())\n",
        "\n",
        "# Visualize the distribution of the target variable.\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(train_df[TARGET_COL], kde=True)\n",
        "plt.title(f'Distribution of {TARGET_COL}')\n",
        "plt.xlabel(TARGET_COL)\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the correlation matrix of a subset of features (to avoid overwhelming visualization).\n",
        "# This can help identify relationships between features.\n",
        "if len(FEATURES) > 10:\n",
        "    subset_cols = FEATURES[:10] + [TARGET_COL]\n",
        "else:\n",
        "    subset_cols = FEATURES + [TARGET_COL]\n",
        "\n",
        "correlation_matrix = train_df[subset_cols].corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of a Subset of Features')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot of a few features against the target variable to visualize relationships.\n",
        "if len(FEATURES) >= 2:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    sns.scatterplot(x=train_df[FEATURES[0]], y=train_df[TARGET_COL], ax=axes[0])\n",
        "    axes[0].set_title(f'{FEATURES[0]} vs {TARGET_COL}')\n",
        "    sns.scatterplot(x=train_df[FEATURES[1]], y=train_df[TARGET_COL], ax=axes[1])\n",
        "    axes[1].set_title(f'{FEATURES[1]} vs {TARGET_COL}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize a few molecules from the  column\n",
        "if MOLECULE_COL in train_df.columns:\n",
        "    molecules = train_df[MOLECULE_COL].unique()[:4]  # Show a maximum of 4 molecules\n",
        "    fig, axes = plt.subplots(1, len(molecules), figsize=(15, 5))\n",
        "    for i, mol_smiles in enumerate(molecules):\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(mol_smiles) # Convert SMILES to Molecule object\n",
        "            if mol:\n",
        "                Chem.Draw.MolToImage(mol, size=(200, 200), ax=axes[i], fitImage=True)\n",
        "                axes[i].set_title(f'Molecule {i+1}')\n",
        "            else:\n",
        "                axes[i].text(0.5, 0.5, f'Invalid SMILES: {mol_smiles}', ha='center', va='center', transform=axes[i].transAxes)\n",
        "        except Exception as e:\n",
        "             axes[i].text(0.5, 0.5, f'Error: {e}', ha='center', va='center', transform=axes[i].transAxes)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ## 4. Feature Engineering\n",
        "\n",
        "# Example of a simple feature engineering step: creating polynomial features.\n",
        "# In a real-world scenario, this is where you'd use RDKit to generate\n",
        "# molecular descriptors.  Since the dummy data does not have molecular information,\n",
        "# we will skip RDKit.\n",
        "try:\n",
        "  poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "  X_poly = poly.fit_transform(train_df[FEATURES])\n",
        "  X_poly_df = pd.DataFrame(X_poly, columns=[f'poly_{i}' for i in range(X_poly.shape[1])])\n",
        "  train_df = pd.concat([train_df, X_poly_df], axis=1)\n",
        "  FEATURES = FEATURES + [col for col in train_df.columns if 'poly_' in col]\n",
        "  print(\"Polynomial features created.\")\n",
        "except Exception as e:\n",
        "  print(f\"Error creating polynomial features: {e}\")\n",
        "\n",
        "# ## 5. Data Preprocessing\n",
        "\n",
        "# Separate features (X) and target (y).\n",
        "X = train_df[FEATURES]\n",
        "y = train_df[TARGET_COL]\n",
        "\n",
        "# Split the data into training and testing sets for initial evaluation.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize numerical features. This is important for linear models and some tree-based models.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert scaled data back to DataFrames for easier handling later if needed.\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=FEATURES)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=FEATURES)\n",
        "\n",
        "# ## 6. Model Training and Evaluation\n",
        "\n",
        "# We will train and evaluate several regression models using cross-validation\n",
        "# to get a more robust estimate of their performance on unseen data.\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scoring_metric = 'neg_mean_absolute_error' # Using negative MAE as cross_val_score maximizes\n",
        "\n",
        "def evaluate_model(model, X, y, cv, scoring):\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
        "    mae_scores = -scores # Convert negative MAE back to positive\n",
        "    print(f\"{type(model).__name__}: Mean MAE = {mae_scores.mean():.4f}, Std MAE = {mae_scores.std():.4f}\")\n",
        "    return mae_scores.mean()\n",
        "\n",
        "# ### 6.1. Linear Regression\n",
        "\n",
        "linear_reg = LinearRegression()\n",
        "evaluate_model(linear_reg, X_train_scaled, y_train, cv, scoring_metric)\n",
        "\n",
        "# ### 6.2. Ridge Regression (L2 Regularization)\n",
        "\n",
        "ridge_reg = Ridge(alpha=1.0) # You can tune alpha\n",
        "evaluate_model(ridge_reg, X_train_scaled, y_train, cv, scoring_metric)\n",
        "\n",
        "# ### 6.3. Lasso Regression (L1 Regularization)\n",
        "\n",
        "lasso_reg = Lasso(alpha=0.01) # You can tune alpha\n",
        "evaluate_model(lasso_reg, X_train_scaled, y_train, cv, scoring_metric)\n",
        "\n",
        "# ### 6.4. Polynomial Regression (to capture non-linear relationships)\n",
        "\n",
        "poly_pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('linear', LinearRegression())\n",
        "])\n",
        "evaluate_model(poly_pipeline, X_train, y_train, cv, scoring_metric)\n",
        "\n",
        "# ### 6.5. Random Forest Regressor\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "evaluate_model(rf_reg, X_train, y_train, cv, scoring_metric)\n",
        "\n",
        "# ### 6.6. Gradient Boosting Regressor\n",
        "\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "evaluate_model(gb_reg, X_train, y_train, cv, scoring_metric)\n",
        "\n",
        "# ### 6.7. LightGBM Regressor\n",
        "\n",
        "lgbm_reg = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, n_jobs=-1)\n",
        "evaluate_model(lgbm_reg, X_train, y_train, cv, scoring_metric)\n",
        "\n",
        "# ### 6.8. XGBoost Regressor\n",
        "\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, n_jobs=-1)\n",
        "evaluate_model(xgb_reg, X_train, y_train, cv, scoring_metric)\n",
        "\n",
        "# ### 6.9. Voting Regressor (Ensemble of diverse models)\n",
        "\n",
        "voting_reg = VotingRegressor(estimators=[\n",
        "    ('lr', linear_reg),\n",
        "    ('ridge', ridge_reg),\n",
        "    ('lasso', lasso_reg),\n",
        "    ('rf', rf_reg),\n",
        "    ('gb', gb_reg),\n",
        "    ('lgbm', lgbm_reg),\n",
        "    ('xgb', xgb_reg)\n",
        "])\n",
        "evaluate_model(voting_reg, X_train_scaled, y_train, cv, scoring_metric)\n",
        "\n",
        "# ## 7. Model Evaluation on the Test Set\n",
        "\n",
        "# Let's evaluate the performance of one of the promising models (e.g., Gradient Boosting)\n",
        "# on the held-out test set.\n",
        "\n",
        "final_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "final_model.fit(X_train, y_train)\n",
        "predictions = final_model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"\\nPerformance of Gradient Boosting Regressor on the Test Set: MAE = {mae:.4f}\")\n",
        "\n",
        "# Visualize the predictions against the actual values on the test set.\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_test, y=predictions)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs. Predicted Values (Gradient Boosting on Test Set)')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Diagonal line for perfect predictions\n",
        "plt.show()\n",
        "\n",
        "# Visualize the residuals (the difference between actual and predicted values).\n",
        "residuals = y_test - predictions\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title('Distribution of Residuals (Gradient Boosting on Test Set)')\n",
        "plt.xlabel('Residuals (Actual - Predicted)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# ## 8. Feature Importance\n",
        "\n",
        "# Display feature importance for a tree-based model (e.g., Gradient Boosting)\n",
        "if hasattr(final_model, 'feature_importances_'):\n",
        "    feature_importance = final_model.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
        "    plt.title('Feature Importance (Gradient Boosting)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nFeature importance is not available for the final model.\")\n",
        "\n",
        "# ## 9. Analysis of Errors\n",
        "# Get more detail on model errors\n",
        "def analyze_errors(y_true, y_pred, X,  top_n=10):\n",
        "\n",
        "    errors = y_true - y_pred\n",
        "    error_df = pd.DataFrame({'Error': errors, 'Actual': y_true, 'Predicted': y_pred})\n",
        "    X_with_errors = pd.concat([X.reset_index(drop=True), error_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Display largest errors\n",
        "    largest_errors = X_with_errors.nlargest(top_n, 'Error')\n",
        "    print(f\"\\nTop {top_n} Largest Errors:\")\n",
        "    print(largest_errors)\n",
        "\n",
        "    # Display smallest errors\n",
        "    smallest_errors = X_with_errors.nsmallest(top_n, 'Error')\n",
        "    print(f\"\\nTop {top_n} Smallest Errors:\")\n",
        "    print(smallest_errors)\n",
        "\n",
        "    # Scatter plot of predicted vs actual, with errors color-coded\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(y_true, y_pred, c=errors, cmap='coolwarm', alpha=0.7)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title('Predicted vs. Actual Values with Errors')\n",
        "    plt.colorbar(label='Error (Actual - Predicted)')\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')  # Diagonal line\n",
        "    plt.show()\n",
        "    return X_with_errors\n",
        "\n",
        "error_analysis_df = analyze_errors(y_test, predictions, X_test)\n",
        "\n",
        "# ## 10. Further Steps (Indicating Continued Interest)\n",
        "\n",
        "print(\"\\nFurther Steps to Explore:\")\n",
        "print(\"- More extensive feature engineering based on molecular properties using RDKit (if available in the actual dataset).  Example:  calculate molecular weight, number of rings, etc.\")\n",
        "print(\"- Hyperparameter tuning of the individual models and the Voting Regressor using techniques like GridSearchCV or RandomizedSearchCV.  Example: tune the number of trees, learning rate, and max depth for Gradient Boosting.\")\n",
        "print(\"- Exploring more advanced ensemble methods like Stacking, where a meta-model learns how to combine the predictions of the base models.\")\n",
        "print(\"- Investigating the use of dimensionality reduction techniques (e.g., PCA, t-SNE) if the number of features is very high after feature engineering, to reduce complexity and potentially improve performance.\")\n",
        "print(\"- If molecular structure information is available (e.g., SMILES strings), using cheminformatics libraries like RDKit to generate molecular descriptors.  Example:  calculate more complex molecular descriptors and fingerprints.\")\n",
        "print(\"- Analyzing the types of errors made by the models to gain insights for improvement.  Are there specific types of molecules or specific ranges of coupling constants where the model performs poorly?\")\n",
        "print(\"- Experimenting with different ways of handling the molecular structure data, such as using graph neural networks (GNNs) to directly learn from the molecular graph representation.\")\n",
        "print(\"-  Consider using different cross-validation strategies, such as GroupKFold, if there are known dependencies between data points (e.g., if multiple rows come from the same molecule).\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "CSVvMYoW6BHo"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}